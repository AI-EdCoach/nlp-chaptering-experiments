P-tune — если кратко, это автоматизированный подбор промпта (более детально об этом рассказал коллега в этой статье). Он работает эффективнее, чем человек, которого посадили подбирать удачный промпт. В целом это хороший подход, но в нашем случае он почти никогда не давал требуемых результатов с необходимым качеством. Сам подход не может именно научить модель: задача должна быть достаточно простой для модели или в каком-то виде присутствовать в обучении.

Для сложных задач интереснее LoRA и fine-tune. На практике разница между ними в максимальном качестве, которого можно достичь: у разморозки слоёв в fine-tune предел качества выше. Но давайте рассмотрим каждый подход подробнее.

У LoRA есть параметр — ранг. Чем больше ранг, тем более сложному навыку можно обучить модель. Но и тут есть свои нюансы. Мы проводили эксперимент с рангом LoRA и ростом качества в зависимости от него. Шаг ранга — степень двойки (4, 8, 16, 32, 64, 128, 256, 512). Спешу расстроить: экспоненциального роста качества модели, увы, не происходит.

Мы выяснили, что лучше выбирать значение ранга 8 или 32. Ранг 8 — золотая середина, но с моделью, у которой изначально было низкое качество или неудачная архитектура, вы, скорее всего, быстро упрётесь в потолок. При ранге 32 модель начинает работать лучше и улавливать более мелкие детали. С дальнейшим увеличением ранга можно увидеть рост метрик, но по субъективным оценкам качество падает: модель начинает цепляться за ошибки в обучающих данных и чаще галлюцинировать.

Обучение LoRA требует от 1000 до 50 000 примеров для обучения. На большем количестве обучающих примеров на наших данных мы не получали роста качества, хотя до этого была очевидная зависимость качества от количества данных.

В случае fine-tune можно обучать модель целиком — full fine-tune. Для больших моделей такой вариант достаточно требователен к ресурсам, поэтому в качестве альтернативы можно обучать только часть слоёв. Обычно это называют «разморозкой»: модель по умолчанию «заморожена» и большинство весов зафиксированы. Если «разморозить» только необходимые слои, обучение будет быстрее, но проблемнее — спрогнозировать, какие слои размораживать, достаточно сложно. Full fine-tune позволяет получить максимальное качество на текущей архитектуре модели, но этот метод более требователен к количеству и качеству данных, чем LoRA и p-tune.

Теперь мы знаем все плюсы и минусы всех подходов — что же выбрать? При малом количестве данных использование fine-tune приведёт к тому, что модель будет более подвержена prompt injection — исходя из наших наблюдений, с LoRA такое случается реже. В то же время при fine-tune потенциал качества выше — модель начинает улавливать больше интересных деталей. Кстати, это одновременно и плюс, и минус: при недостаточно качественном датасете модель обязательно научится чему-то плохому даже из всего лишь из пары примеров. В случае с LoRA с рангом 8 или 32 есть шанс, что такого не произойдёт. Однако, если есть идеальный датасет, то full fine-tune даст наибольшее качество.

Для нас LoRa оказался эффективным препродакшн-инструментом для проведения множества экспериментов при подготовке датасета и проверке гипотез. Это метод позволяет быстро и дёшево исследовать различные подходы и идеи, не тратя много времени и ресурсов. Однако мы понимаем, что LoRa — неидеальное решение. Поэтому после того, как мы упираемся в потолок качества, переход на fine-tune даёт нам рост в качестве за счёт того, что у нас есть хорошо подготовленный качественный датасет.

В целом сочетание LoRa и fine-tune позволяет нам эффективно использовать преимущества обоих методов для достижения лучших результатов. Такой подход мы применили для суммаризации страниц, и аналогичный подход применялся для пересказа видео.